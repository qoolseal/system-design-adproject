## Обоснование выбора технологий

### Реляционные базы данных (PostgreSQL)
Используется для хранения критичных и транзакционных данных:
- Пользователи, аккаунты, профили.
- Объявления (карточка как факт: владелец, цена, состояние).
- Заказы и оплаты (денежные операции, статусы).
- Справочники (категории, города, тарифы).
- Аудит действий пользователей.

**Почему именно PostgreSQL**:
- Поддержка транзакций и целостности данных (ACID).
- Гибкая система индексов, внешние ключи.
- Возможности партиционирования таблиц для масштабирования.
- Хорошо работает в мультизонной архитектуре.
### NoSQL базы данных

**Elasticsearch**:
- Для полнотекстового поиска и фильтрации объявлений.
- Быстрая выдача результатов и поддержка геопоиска.
- Репликация и шардирование встроены.
- Документ: `ad_id`, `title`, `description`, `price`, `currency`, `category`, `city`, `status`, `created_at`, `updated_at`, `owner_rating`, `location: geo_point`, производные поля для сортировок.
- Карта полей: `title/description` — полнотекст, `category/city/status` — keyword, `price/owner_rating` — number, `location` — geo_point.
- Обновление: из PostgreSQL через `kafka → search-indexer → ES`.

**ClickHouse**:
- Для аналитики и отчётности (просмотры, конверсии, метрики).
- Эффективное хранение и обработка больших объёмов событий.
- Поддержка материализованных представлений и партиционирования.
- Таблица `events`:
    - Колонки: `event_time DateTime`, `event_type LowCardinality(String)`, `user_id UUID`, `ad_id UUID`, `order_id UUID nullable`, `meta_json String`
    - Партиция: по месяцу `toYYYYMM(event_time)`
    - Сортировка: `(event_type, ad_id, user_id, event_time)`
    - TTL для сырых событий (например, 365 дней), при необходимости — материализованные представления для витрин (DAU/WAU, конверсии, воронки).
- Витрины (`events_daily`, `orders_stats`) заполняются материализованными представлениями.
### Кэш (Redis)
- Быстрый доступ к часто используемым данным (сессии, профили, списки, счётчики).
- Поддержка кластерного режима, Sentinel, механизмов репликации.
- Возможность атомарных операций и Lua-скриптов для консистентности.
### CDN
- Раздача статических файлов и медиа 
	- **Медиа**: фото/видео объявлений, аватары, превью, иконки.
	- **Статика фронтенда**: JS/CSS/шрифты, изображения интерфейса.
- Снижение нагрузки на бэкенд, ускорение загрузки.
- Геораспределённые узлы обеспечивают минимальную задержку.
## Репликация и шардирование
### PostgreSQL
- **Репликация внутри Москвы:** одна реплика синхронная в другой зоне (минимальный риск потери данных при локальной аварии), ещё одна — асинхронная для чтения/отчётов.
- **Партиционирование:** `orders`, `payments`, `audit_log` — по времени; это ускоряет очистку/архив и крупные запросы.
- **Индексы:** покрывающие и частичные индексы под самые частые запросы (по статусам, владельцам, времени).
- **Шардирование (план на рост):**
    - Порог включения — когда одна мастер-база упирается в пределы по CPU/IO/размеру.
    - Ключ шардирования: `user_id` (равномерность) или функционально (разделить домены: «заказы/платежи» отдельно).
    - Маршрутизация на уровне приложений по ключу; общие справочники — в отдельной базе (без шардинга).
- **DR (Новосибирск):** регулярные бэкапы и архивы журналов изменений (WAL) копируются в хранилище с доступом из НСК; при потере Москвы — разворачиваем базу из бэкапов по инструкции.
### Elasticsearch
- **Шардирование:** число шардов подбирается из расчёта ожидаемого объёма индекса и роста (например, 3–5 шардов), **реплики = 1–2** (по зонам).
- **ILM/ретеншн:** политика жизненного цикла индексов (горячие → тёплые → удаление или снапшот).
- **DR:** регулярные снапшоты индексов в объектное хранилище, доступное из НСК.
### ClickHouse
- **Репликация:** по зонам в Москве (ReplicatedMergeTree).
- **Шардирование:** по времени + опционально по `ad_id`/`user_id` при росте объёма; агрегации — через распределённые таблицы.
- **DR:** бэкапы и/или синхронизация с объектным хранилищем, откуда можно поднять кластер в НСК.
## Стратегия консистентности и управления кэшем

### Что кэшируем
- **Популярные страницы и списки** (результаты поиска, ленты категорий/городов).  
    TTL: 30–120 сек, отдельные ключи на каждый набор фильтров и сортировок.
- **Карточки объявлений** (часто смотрят).  
    TTL: 10–30 мин; прогрев при старте search‑service.
- **Профиль пользователя, настройки, права**.  
    TTL: 5–15 мин; инвалидация по событию `UserProfileUpdated`.
- **Справочники** (категории, города, тарифы).  
    TTL: 1–24 ч; инвалидация при изменении записи.
- **Внешние API** (доставка, тарифы партнёров).  
    TTL: 15–30 мин; обновление «по запросу», чтобы не ходить к партнёру каждый раз.
### Подходы
- **Cache-aside (read-through)** для объявлений, списков и справочников.
- **Write-through** для профилей: изменения сразу пишутся в БД и кэш.
- **Отложенная запись** для счётчиков (инкременты в Redis, периодический сброс в БД).
- **Политика вытеснения**: `allkeys-lru`, лимит на общий объём, запрет слишком больших значений (дробить на части).
### Управление кэшем
- Инвалидация по событиям (UserProfileUpdated, AdUpdated).
- Версионирование пространств ключей для списков (например, search:{ver}:{hash}).
- Разброс TTL, чтобы избежать массового истечения ключей одновременно.
- Fallback: при недоступности Redis — прямое чтение из БД/ES.
### Консистентность
- Источник истины — PostgreSQL.
- Redis используется как оптимизация скорости, данные там считаются временными.
- Все операции изменения фиксируются в БД и через Outbox публикуются в Kafka.
- Обработчики событий обновляют кэш и индекс поиска (ES).
## ## Связь БД, кэша и CDN 
**Загрузка медиа:**
1. Пользователь загружает файл → сервис валидации и антивирусной проверки (media-service).
2. Сохраняем оригинал в объектное хранилище (Москва), генерируем варианты (S/M/L/WebP/AVIF).
3. В БД фиксируем метаданные и публичные/подписанные URL.
4. Публикуем событие «медиа готово» → фронт может обновить карточку.

**Отдача страницы/карточки:**
1. Динамика (название, цена, статус) — из БД/кэша.
2. Список/поиск — из Elasticsearch (с кэшированием ответа в Redis).
3. Изображения и статика — напрямую с CDN (кэш на «краях» сети).

**Обновление карточки/медиа:**
1. Изменение в БД → событие → инвалидация ключей Redis (карточка, списки).
2. Если заменили изображение — новая версия файла (новое имя/URL), фронт получает свежую ссылку; CDN продолжает хранить старую до истечения TTL, это не мешает.

**DR‑сценарий:**
- Медиа уже в НСК‑хранилище (копии). После развёртывания в НСК меняем origin у CDN (или используем резервный домен), система начинает раздавать медиа из резервной площадки.
## План масштабирования системы

### PostgreSQL
- В Москве: ведущий сервер и несколько реплик по зонам (синхронная и асинхронная).
- Партиционирование больших таблиц (orders, payments, audit).
- При росте нагрузки — горизонтальное шардирование по ключу (user_id или бизнес-домен).
- Регулярные бэкапы и WAL-архивирование → хранилище, доступное в Новосибирске.

### Elasticsearch
- Индекс объявлений разбит на несколько шардов (3–5).
- Реплики = 1–2 для отказоустойчивости.
- Политики ILM для «старых» индексов: перевод в тёплое хранилище или удаление.
- Снапшоты индексов в объектное хранилище.

### ClickHouse
- ReplicatedMergeTree по зонам в Москве.
- Партиционирование по времени.
- Возможность добавления новых шардов при росте нагрузки.
- TTL для сырых событий, агрегации через материализованные представления.

### Redis
- Кластерный режим с Sentinel.
- Возможность горизонтального масштабирования по шардам.
- Регулярные снапшоты (RDB/AOF) в хранилище.

### CDN
- Большие TTL + версионирование файлов 
- Ленивая загрузка изображений.
- Использование современных форматов (WebP/AVIF).
- При росте трафика — автоматическое масштабирование через сеть CDN.

## Итог

Предложенная архитектура объединяет:
- **Реляционное ядро (PostgreSQL)** для транзакционной целостности.
- **Поисковый индекс (Elasticsearch)** для быстрых запросов.
- **Аналитическое хранилище (ClickHouse)** для отчётности.
- **Кэш (Redis)** для ускорения операций чтения и снижения нагрузки на БД.
- **CDN** для раздачи статики и медиа.

Такая комбинация обеспечивает высокую производительность, масштабируемость и надёжность, при этом оптимизирует затраты за счёт разграничения «источника истины» и вспомогательных ускорителей.
