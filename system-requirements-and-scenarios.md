## Работа с требованиями системы
### Согласованность
- **Источник истины — PostgreSQL.** Все изменения фиксируются транзакциями (ACID), уникальные ограничения и внешние ключи защищают инварианты.
- **Кэш и поиск — «ускорители».** Redis и Elasticsearch держат производные данные и **допускают eventual consistency** (небольшую задержку обновления).
- **Событийная модель.** После фиксации в БД изменение попадает в таблицу `outbox`, затем в Kafka, а оттуда — в потребители: обновление кэша и индекса поиска.
- **Идемпотентность.** Повторные события обрабатываются без дублирования (идемпотентные ключи/версии объектов).
- **Границы согласованности.** Платежи/создание заказов — строго согласованные (CP); поиск/каталоги — допускают небольшое отставание (AP).

### Доступность
- **Один ЦОД в Москве, несколько зон.** PostgreSQL, Kafka, Redis, Elasticsearch разнесены по зонам: отказ одной зоны не останавливает систему.
- **Автоперезапуск и самовосстановление.** Kubernetes перезапускает поды; Patroni переключает ведущий в кластере БД; у Kafka — репликация и кворум брокеров.
- **Деградация без остановки.** Если Redis недоступен — читаем напрямую из БД/ES; если ES перегружен — отвечаем «упрощённым» набором полей или старым кэшем.
- **CDN для статики/медиа.** Даже при частичных сбоях серверов пользователи получают картинки и фронтовую статику.

### Производительность
- **Где что исполняем.** Пишем — в PostgreSQL; ищем и фильтруем — в Elasticsearch; тяжёлую аналитику — в ClickHouse; «горячие» данные — в Redis.
- **Оптимизация БД.** Индексы под частые запросы, партиционирование больших таблиц, пул соединений (pgbouncer), анализ `EXPLAIN ANALYZE`.
- **Асинхронные цепочки.** Долгие операции — через очередь событий; фронту возвращаем результат быстро, обработку продолжаем в фоне.
- **Ограничение нагрузки.** Таймауты, ретраи с «джиттером», ограничение RPS и пула соединений к БД, плавная деградация.

## Как держим пики нагрузки (примерные сценарии)
### 1. Всплеск чтения (много просмотров объявлений)
**Ситуация:**  
Например, вечером в пятницу на сайт заходит в 10 раз больше пользователей, чем обычно. Они в основном смотрят ленту и карточки объявлений.

**Что будет:**
- Пользователи активно грузят списки и карточки → основной поток обращений — на чтение.
- База данных PostgreSQL могла бы «захлебнуться», если бы к ней ходили напрямую.

**Что делает система:**
- **Redis** уже содержит «горячие» карточки и списки. Если данные есть в кэше → ответ отдаётся за миллисекунды.
- **Elasticsearch** обслуживает поисковые запросы (фильтры, сортировки), чтобы не нагружать PostgreSQL.
- **CDN** раздаёт картинки и фронтенд-статику без участия серверов.
- **Автоскейлер Kubernetes** поднимает больше pod’ов чтения (search-service, ad-service), чтобы распределить нагрузку.

**Для пользователя:** сайт остаётся быстрым, хотя в бекенде нагрузка выросла в разы.
### 2. Всплеск записи (массовые изменения/акции)

**Ситуация:**  
Компания запустила акцию, пользователи массово подают объявления. В секунду приходит не 100, а 2000 запросов на создание объявлений.

**Что будет:**
- Основная нагрузка — на запись в PostgreSQL (новые объявления и заказы).
- Если все пойдут напрямую, база может начать отклонять соединения.

**Что делает система:**
- **Ограничение RPS**: фронтовые сервисы выставляют лимиты на записи, чтобы база не падала; при превышении выдаём «429 — попробуйте позже».
- **Kafka и outbox**: запросы сначала пишутся в «журнал событий», подтверждаются пользователю (данные сохранены), а «тяжёлая обработка» (обновление поиска, аналитика) идёт асинхронно.
- **Приоритеты:** транзакции (оплаты, заказы) идут сразу в базу, фоновые операции (обновления счётчиков просмотров, логирование) могут откладываться.

**Для пользователя:** объявление или заказ сохраняются, иногда с задержкой появляются в поиске, но сайт не «лежит».
### 3. Пиковые кампании (праздники, маркетинговые акции)

**Ситуация:**  
Перед Новым годом или «Чёрной пятницей» ожидается резкий наплыв. В течение пары часов онлайн увеличивается в 5–10 раз.

**Что будет:**
- Одновременно много просмотров и много записей.
- Система должна выдержать без массовых ошибок.

**Что делает система заранее:**
- **Прогрев кэша**: популярные списки и карточки заранее кладём в Redis, чтобы их не строить заново.
- **Увеличение ресурсов**: заранее масштабируем pod’ы и реплики БД «вширь».
- **Упрощённый поиск**: при перегрузке можем отключить «дорогие» сортировки или фильтры (например, «сортировать по рейтингу продавца»), чтобы отвечать быстрее.
- **Резервные очереди**: часть фоновых задач (например, аналитика) временно откладывается, приоритет остаётся за заказами и оплатами.

**Для пользователя:** сайт остаётся доступным, но отдельные функции (например, сортировка по редким параметрам) могут работать чуть менее точно

## Расчёты 
### PostgreSQL
- исходя из [НФТ](details-description.md) (пик 18:00-21:00: **1000 RPS**, из них: поиск 700, просмотр карточки 200, оформление заказа+авторизация 100; SLO: 95% заказов <1 сек), рассчитаем нагрузки на БД
	- **Поиск (700 RPS)** → **ES/Redis**. В PostgreSQL почти 0 запросов (только редкие справочники/профили при промахе кэша).
	- **Просмотр объявления (200 RPS)** → целимся в 80-90% кэш-хитов в Redis.  
	  → в PostgreSQL ≈ **20-40 RPS** чтений (10-20% промахов).
	- **Оформление заказа + авторизация (100 RPS)**:  
		- Авторизация сильно кэшируемая (сессии/токены в Redis): пусть 10% промахов → **~4-6 RPS** чтений в PG.  
	- **Создание заказа**: это «дорогая» часть. На один заказ: `orders` + `order_items` + `payments`/`payment_attempts` + `audit_log` + `outbox` (для саги) → **5-8 записей** на один заказ (консервативно). Если заказы - 60 RPS из этих 100, то получаем **~300-480 записей/сек** в PostgreSQL
- Итог в пиковые нагрузки для Postgres:
	- чтение ~30-50 RPS
	- записи ~300-480 write/s
	- эта нагрузка для 1 мастер-реплики + 2 копий рабочая, поэтому пока что шардирование не нужно
- когда шардирование понадобится:
	- на мастере TPS превысит 1-2к
	- p95 пишущих транзакций превысит 400-600мс
#### объём данных
- расчёт по заказам: конверсию можем только предполагать, рассмотрим несколько сценариев:
	- **S1 (консервативный)**: конверсия во время пика ~1% от 200 RPS просмотров карточек ⇒ **~2 RPS заказов в пик**; вне пика ~0.5 RPS.  
	   `2*10800 + 0.5*75600 ≈ 32 тыс. заказов/сутки`.
    
	- **S2 (средний)**: ~5% в пик ⇒ **~10 RPS**; вне пика ~2 RPS.  
	   `10*10800 + 2*75600 ≈ 259 тыс./сутки`.
    
	- **S3 (агрессивный)**: как мы прикидывали раньше — **60 RPS в пик**, вне пика ~6 RPS.  
	  `60*10800 + 6*75600 ≈ 1.10 млн/сутки`.
- размеры данных в Postgres
	- `orders` — ~1.0 KB (данные) + ~1.0 KB (индексы) = **~2.0 KB**
	- `order_items` — 2 позиции по ~0.5 KB (данные+индексы) = **~1.0 KB**
	- `payments`/`payment_attempts` = **~1.0 KB** (суммарно)
	- `audit_log` — 3 записи × 0.3 KB = **~0.9 KB**
	- `outbox` — 2 события × 0.3 KB = **~0.6 KB**
- Итого данных+индексов на один заказ ≈ 5.5 KB. С учётом WAL/репликации и накладных расходов (коэф. 1.2–1.5) закладываем **~7–8.5 KB/заказ**

- Суточный объём записи в PostgreSQL (данные+индексы)
	- **S1 (~32 тыс./сутки)** → 32k × 5.5 KB ≈ **176 MB/день** (WAL ~ 210–260 MB)
		- ~1.0 млн строк/мес - или 5,5 GB/мес
	- **S2 (~259 тыс./сутки)** → ≈ **1.4 GB/день** (WAL ~ 1.7–2.1 GB)
		- ~8.0 млн/мес → ~44 GB/мес
	- **S3 (~1.10 млн/сутки)** → ≈ **6.0 GB/день** (WAL ~ 7.2–9.0 GB)
		- ~33 млн/мес → ~180 GB/мес
	- Вывод: для S1 и S2 Postgres будет спокойно справляться. Мы партиционируем данные по времени (месяцам) - так что через 12-18 месяцев можно начинать архивировать старые данные (старше года), к этому времени объём данных дойдёт примерно до 1 ТБ
### Redis
##### Карточки (ad-cache)
- Пик 200 RPS, промахи 10–20% при TTL 10–30 минут.
- Уникальных ключей за 10 минут: 200 × 0.2 × 600 ≈ **24k ключей**.
- 1 карточка ≈ 1–2 KB JSON + метаданные ключа/TTL → **~2–3 KB/ключ**.
- **Итого 50–75 MB** под самые популярные карточки

##### Поисковые результаты
- 700 RPS, TTL 30–120 сек, доля уникальных запросов ~30%.
- За 60 сек: 700 × 0.3 ≈ **210 ключей/мин**.
- Если каждый поисковой результат (пагинация/топ N) ≈ **10–30 KB**.
- То за 1–2 мин в кэше окажется **~2–12 MB**.

С запасом под сессии/профили/счётчики и прочее **~2–4 GB RAM** на кластер Redis достаточно
### Elasticsearch
- Средний документ (объявление) после индексации и сжатия занимает **2–5 КБ**.
- В индексе всегда есть 1 основная копия и 1–2 реплики (для отказоустойчивости).
- При 10 млн объявлений: `10 000 000 × 2–5 КБ = 20–50 ГБ`, с учётом репликации - 40-100 Гб
- Горячими (в SSD) можно хранить только 20-30 Гб из них